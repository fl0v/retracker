#!/bin/bash

# Script to download tracker lists, merge with local curated list, and regenerate forwarders.yml
# Only keeps HTTP trackers and removes duplicates

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
LISTS_DIR="${SCRIPT_DIR}/lists"
CUSTOM_LIST="${LISTS_DIR}/_custom.txt"
OUTPUT_FILE="${PROJECT_ROOT}/configs/forwarders.yml"

# Define tracker list sources (URL and local filename)
# Format: "URL|filename"
TRACKER_LISTS=(
    "https://newtrackon.com/api/stable|newtrackon.txt"
    "https://trackerslist.com/best.txt|trackerslist_best.txt"
    "https://trackerslist.com/http.txt|trackerslist_http.txt"
    "https://raw.githubusercontent.com/ngosang/trackerslist/master/trackers_best.txt|ngosang_best.txt"
)

# Internal processing files to skip (files starting with underscore)
SKIP_FILES=("_all.txt" "_normalized.txt" "_unique.txt" "_custom.txt" "forwarders.yml")

# Create lists directory if it doesn't exist
mkdir -p "$LISTS_DIR"
# Create configs directory if it doesn't exist
mkdir -p "${PROJECT_ROOT}/configs"

echo "Downloading tracker lists to ${LISTS_DIR}..."

# Download online lists (with timeout and error handling)
for list_entry in "${TRACKER_LISTS[@]}"; do
    IFS='|' read -r url filename <<< "$list_entry"
    output_file="${LISTS_DIR}/${filename}"
    echo "  Downloading ${filename}..."
    curl -s -L --max-time 10 "$url" -o "$output_file" 2>/dev/null || echo "Warning: Failed to download ${filename} from ${url}" >&2
done

# Function to extract HTTP trackers from a file
extract_http_trackers() {
    local file="$1"
    if [[ ! -f "$file" ]] || [[ ! -s "$file" ]]; then
        return
    fi
    
    # Extract HTTP/HTTPS URLs
    # Handle formats: one per line (most common), comma-separated, space-separated, JSON arrays
    # First, extract lines starting with http:// or https://
    grep -ihE '^https?://' "$file" 2>/dev/null | \
    sed 's/[[:space:]]*$//' | \
    # Also extract URLs from within lines (for comma/space separated or JSON)
    cat - <(grep -ihEo 'https?://[^[:space:],\[\]"]+' "$file" 2>/dev/null) | \
    sed 's/[[:space:]]*$//' | \
    sed 's/,$//' | \
    sed 's/\]$//' | \
    sed 's/\[$//' | \
    sed 's/"$//' | \
    sed 's/^[[:space:]]*"//' | \
    sed 's/[[:space:]]*$//' | \
    grep -iE '^https?://' | \
    sed 's|/$||' | \
    sed 's|/announce$||' | \
    awk '{if (length($0) > 0) print $0 "/announce"}' | \
    sort -u
}

# Extract HTTP trackers from all downloaded files
echo "Extracting HTTP trackers..."
ALL_TRACKERS="${LISTS_DIR}/_all.txt"
> "$ALL_TRACKERS"

for file in "${LISTS_DIR}"/*.txt; do
    filename=$(basename "$file")
    # Skip internal processing files
    skip=false
    for skip_file in "${SKIP_FILES[@]}"; do
        if [[ "$filename" == "$skip_file" ]]; then
            skip=true
            break
        fi
    done
    [[ "$skip" == true ]] && continue
    
    if [[ -f "$file" ]] && [[ -s "$file" ]]; then
        extract_http_trackers "$file" >> "$ALL_TRACKERS" 2>/dev/null || true
    fi
done

# Add custom curated list if it exists
if [[ -f "$CUSTOM_LIST" ]] && [[ -s "$CUSTOM_LIST" ]]; then
    echo "Adding custom curated trackers..."
    extract_http_trackers "$CUSTOM_LIST" >> "$ALL_TRACKERS" 2>/dev/null || true
else
    echo "Note: Custom curated list not found or empty at ${CUSTOM_LIST}"
    echo "Creating empty file for future use..."
    touch "$CUSTOM_LIST"
fi

# Normalize and deduplicate trackers
echo "Normalizing and deduplicating trackers..."
NORMALIZED="${LISTS_DIR}/_normalized.txt"
> "$NORMALIZED"

if [[ -s "$ALL_TRACKERS" ]]; then
    while IFS= read -r tracker || [[ -n "$tracker" ]]; do
        # Skip empty lines
        [[ -z "$tracker" ]] && continue
        
        # Normalize: convert to lowercase, remove trailing slashes, ensure /announce suffix
        normalized=$(echo "$tracker" | tr '[:upper:]' '[:lower:]' | sed 's|/$||' | sed 's|/announce$||')
        
        # Only process if it's HTTP/HTTPS and not empty
        if [[ "$normalized" =~ ^https?:// ]] && [[ -n "$normalized" ]]; then
            echo "${normalized}/announce" >> "$NORMALIZED"
        fi
    done < "$ALL_TRACKERS"
fi

# Sort and remove duplicates
UNIQUE_TRACKERS="${LISTS_DIR}/_unique.txt"
if [[ -s "$NORMALIZED" ]]; then
    sort -u "$NORMALIZED" > "$UNIQUE_TRACKERS"
else
    > "$UNIQUE_TRACKERS"
fi

# Count trackers
TRACKER_COUNT=$(wc -l < "$UNIQUE_TRACKERS" 2>/dev/null | tr -d ' ' || echo "0")
echo "Found ${TRACKER_COUNT} unique HTTP trackers"

# Generate forwarders.yml
echo "Generating ${OUTPUT_FILE}..."
{
    echo "# Forwarder configuration for retracker"
    echo "# Auto-generated by update-forwarders.sh on $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
    echo "# Total trackers: ${TRACKER_COUNT}"
    echo "# Note: retracker only supports HTTP trackers"
    echo ""
    
    if [[ -s "$UNIQUE_TRACKERS" ]]; then
        while IFS= read -r tracker || [[ -n "$tracker" ]]; do
            [[ -z "$tracker" ]] && continue
            echo "- uri: ${tracker}"
        done < "$UNIQUE_TRACKERS"
    fi
} > "$OUTPUT_FILE"

echo "Successfully generated ${OUTPUT_FILE} with ${TRACKER_COUNT} trackers"
echo "Done!"
